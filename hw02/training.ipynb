{"cells":[{"cell_type":"code","source":"import tensorflow as tf","metadata":{"tags":[],"cell_id":"f3773c2fbda3460ba75cc4bc0d0486e1","source_hash":"7a93dab8","execution_start":1671395674146,"execution_millis":1868,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-18 20:34:34.107497: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-18 20:34:34.252574: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-12-18 20:34:34.257677: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-12-18 20:34:34.257696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-18 20:34:34.280874: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2022-12-18 20:34:34.857904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2022-12-18 20:34:34.857965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2022-12-18 20:34:34.857971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\n\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"tags":[],"cell_id":"0a0b751af0da4327b48e57fb1d30e0cd","source_hash":"6148f199","execution_start":1671395676014,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"MODEL_NAME = 'model4'\n\nPREPROCESSING_ARGS = {\n    'downsampling_rate' : 16000,\n    'frame_length_in_s' : 0.032,\n    'frame_step_in_s' : 0.032,\n    'num_mel_bins': 10,  \n    'lower_frequency' : 20,  \n    'upper_frequency' : 8000,  \n    'num_coefficients' : 10\n}\n\nFILTERS = 128  # filters used in the convolution\n\nalpha = 0.5\n\nLABELS = ['stop', 'go']  # ATTENTION, MUST BE SAME ORDER\n\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n    'epochs': 10\n}\n\nfinal_sparsity = 0.7","metadata":{"tags":[],"cell_id":"1121ee562ff7484e819efe48bb388906","source_hash":"f628d641","execution_start":1671395676015,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from preprocessing import get_mfccs\nfrom functools import partial\n\n# use only go and stop words\ntrain_ds = tf.data.Dataset.list_files(['msc-train/go*', 'msc-train/stop*'])\nval_ds = tf.data.Dataset.list_files(['msc-val/go*', 'msc-val/stop*'])\ntest_ds = tf.data.Dataset.list_files(['msc-test/go*', 'msc-test/stop*'])\n\nget_frozen_mfccs = partial(get_mfccs, **PREPROCESSING_ARGS)\n\n# save the shape of the mfccs\nfor mfccs, label in train_ds.map(get_frozen_mfccs).take(1):\n    SHAPE = mfccs.shape","metadata":{"tags":[],"cell_id":"eff94c6e94f64d7eb365bc92c3b27b6c","source_hash":"b92f6c54","execution_start":1671395676016,"execution_millis":1289,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-18 20:34:35.682933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2022-12-18 20:34:35.682958: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-18 20:34:35.682976: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-919441e0-aa3f-4bac-a9dc-75daa4d8d934): /proc/driver/nvidia/version does not exist\n2022-12-18 20:34:35.683224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-18 20:34:36.394625: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n2022-12-18 20:34:36.394838: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 AVX512F FMA\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-18 20:34:36.777412: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:36.778969: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:36.779149: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def preprocess(filename):\n    signal, label = get_frozen_mfccs(filename)\n\n    signal.set_shape(SHAPE)\n    signal = tf.expand_dims(signal, -1)\n    # signal = tf.image.resize(signal, [32, 32]) # not needed since dimension is smaller than spectogram and it will add more latency\n                                                 # in the inference preprocessing\n    label_id = tf.argmax(label == LABELS)  # change labels from text to a number\n\n    return signal, label_id\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\n# create batches\ntrain_ds = train_ds.map(preprocess).batch(batch_size).cache()\nval_ds = val_ds.map(preprocess).batch(batch_size)\ntest_ds = test_ds.map(preprocess).batch(batch_size)","metadata":{"tags":[],"cell_id":"7bb9f286593b4d248721d661882d3b21","source_hash":"15f7374","execution_start":1671395677306,"execution_millis":1033,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-18 20:34:37.481335: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:37.482877: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:37.483044: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-18 20:34:37.813319: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:37.814689: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:37.814866: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nWARNING:tensorflow:Using a while_loop for converting IO>AudioResample cause there is no registered converter for this op.\n2022-12-18 20:34:38.138048: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:38.139434: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2022-12-18 20:34:38.139610: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"for example_batch, example_labels in train_ds.take(1):\n    pass","metadata":{"tags":[],"cell_id":"f449b468a22046c6803a3f6070183436","source_hash":"86a12536","execution_start":1671395678334,"execution_millis":286,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-18 20:34:38.571829: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# define model\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=int(FILTERS*alpha), kernel_size=[3, 3], strides=[2, 2],  # always start with a standard\n    use_bias=False, padding='valid'),                                        # convolution, but increasing the\n    tf.keras.layers.BatchNormalization(),                                    # number of filter to compensate \n    tf.keras.layers.ReLU(),                                                  # for the loss of parameters\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n    use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=int(FILTERS*alpha), kernel_size=[1, 1], strides=[1, 1],\n    use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],      # don't need the number of filter, because is 1 by\n    use_bias=False, padding='same'),                                         # definition, this is a convolution over the H and W\n    tf.keras.layers.Conv2D(filters=int(FILTERS*alpha), kernel_size=[1, 1], strides=[1, 1],  # point-wise convolution (kernel = [1,1])\n    use_bias=False),                                                         # to do the convolution over the channels C\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n])","metadata":{"tags":[],"cell_id":"983bdd4bbf1f443286d656e2b7e4f621","source_hash":"70dfafbd","execution_start":1671395678635,"execution_millis":88,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# define model for pruning\n\nimport tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude  # rename the function\n\nbegin_step = int(len(train_ds) * epochs * 0.2)  # number of batches * number of epochs is the number of steps\n                                                # we start at 20%\nend_step = int(len(train_ds) * epochs)\n\npruning_params = {\n    'pruning_schedule' : tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.20,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step\n    )  # scheduler\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","metadata":{"tags":[],"cell_id":"046e9f6814a74b9cb7249ffc9d597391","source_hash":"3c855bbd","execution_start":1671395678727,"execution_millis":861,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# train the model\n\nloss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\n\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\n\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks)","metadata":{"tags":[],"cell_id":"5aaf45ef0b0e4471b05d4f7473067fd7","source_hash":"c1f76c6d","execution_start":1671395679594,"execution_millis":24632,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Epoch 1/10\n80/80 [==============================] - 8s 62ms/step - loss: 0.3032 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.3727 - val_sparse_categorical_accuracy: 0.8650\nEpoch 2/10\n80/80 [==============================] - 2s 24ms/step - loss: 0.1475 - sparse_categorical_accuracy: 0.9481 - val_loss: 0.1632 - val_sparse_categorical_accuracy: 0.9550\nEpoch 3/10\n80/80 [==============================] - 2s 23ms/step - loss: 0.0942 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.2615 - val_sparse_categorical_accuracy: 0.9000\nEpoch 4/10\n80/80 [==============================] - 2s 22ms/step - loss: 0.0690 - sparse_categorical_accuracy: 0.9769 - val_loss: 0.2677 - val_sparse_categorical_accuracy: 0.9150\nEpoch 5/10\n80/80 [==============================] - 2s 22ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.2556 - val_sparse_categorical_accuracy: 0.9450\nEpoch 6/10\n80/80 [==============================] - 2s 23ms/step - loss: 0.0460 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.1594 - val_sparse_categorical_accuracy: 0.9450\nEpoch 7/10\n80/80 [==============================] - 2s 23ms/step - loss: 0.0417 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.1506 - val_sparse_categorical_accuracy: 0.9550\nEpoch 8/10\n80/80 [==============================] - 2s 23ms/step - loss: 0.0413 - sparse_categorical_accuracy: 0.9869 - val_loss: 0.1394 - val_sparse_categorical_accuracy: 0.9550\nEpoch 9/10\n80/80 [==============================] - 2s 24ms/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.0855 - val_sparse_categorical_accuracy: 0.9600\nEpoch 10/10\n80/80 [==============================] - 2s 23ms/step - loss: 0.0324 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0774 - val_sparse_categorical_accuracy: 0.9550\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# test model\n\ntest_loss, test_accuracy = model_for_pruning.evaluate(test_ds)","metadata":{"tags":[],"cell_id":"26885e9a7bfc465fa8bbba7241b5fa93","source_hash":"e294f784","execution_start":1671395704227,"execution_millis":825,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"10/10 [==============================] - 1s 38ms/step - loss: 0.0410 - sparse_categorical_accuracy: 0.9850\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# store and print results\n\ntraining_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\nval_loss = history.history['val_loss'][-1]\nval_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Validation Loss: {val_loss:.4f}')\nprint(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","metadata":{"tags":[],"cell_id":"13401e6538034e2ab9c03c89f6ac720e","source_hash":"65d89cfe","execution_start":1671395705036,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Training Loss: 0.0324\nTraining Accuracy: 99.25%\n\nValidation Loss: 0.0774\nValidation Accuracy: 95.50%\n\nTest Loss: 0.0410\nTest Accuracy: 98.50%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# save the model\n\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)  # convert the model\n\n# saved_model_dir = f'./saved_models/{timestamp}'\nsaved_model_dir = f'./saved_models/{MODEL_NAME}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel_for_export.save(saved_model_dir)","metadata":{"tags":[],"cell_id":"7aaf179ca625433dbe13609cd62cac21","source_hash":"e7cb124a","execution_start":1671395705037,"execution_millis":1003,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./saved_models/model4/assets\nINFO:tensorflow:Assets written to: ./saved_models/model4/assets\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# convert model to tflite\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\ntflite_model = converter.convert()","metadata":{"tags":[],"cell_id":"57bbf2e396764d42a26ad7227126e248","source_hash":"93567f0b","execution_start":1671395706041,"execution_millis":588,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2022-12-18 20:35:06.472832: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2022-12-18 20:35:06.472871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2022-12-18 20:35:06.473457: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/model4\n2022-12-18 20:35:06.475777: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2022-12-18 20:35:06.475802: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/model4\n2022-12-18 20:35:06.479778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2022-12-18 20:35:06.481129: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2022-12-18 20:35:06.514866: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/model4\n2022-12-18 20:35:06.524505: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 51050 microseconds.\n2022-12-18 20:35:06.544385: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"tflite_models_dir = './tflite_models'\n\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"tags":[],"cell_id":"6aa57d972d544879b65428b80b2929ed","source_hash":"878a830c","execution_start":1671395706641,"execution_millis":53,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\n\nwith open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","metadata":{"tags":[],"cell_id":"2364e83d0c574dc59eed8cc25222a23e","source_hash":"a7f3e42a","execution_start":1671395706695,"execution_millis":0,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# compress tflite model\n\nimport zipfile\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)","metadata":{"tags":[],"cell_id":"05237fe7739a486f9a37216adc209717","source_hash":"667c966e","execution_start":1671395706696,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":16},{"cell_type":"code","source":"original_model_size = os.path.getsize(f'./saved_models/{MODEL_NAME}/saved_model.pb') / 1024\npruned_tflite_size = os.path.getsize(tflite_model_name) / 1024\npruned_zip_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024\n\nprint(f'Original Size (pruned model): {original_model_size:.2f} KB')\nprint(f'Original TFLite Size (pruned model): {pruned_tflite_size:.2f} KB')\nprint(f'ZIP TFLite Size (pruned model): {pruned_zip_size:.2f} KB')","metadata":{"tags":[],"cell_id":"085608a5da814dc9bdd37ed0fc7b86e8","source_hash":"5571241f","execution_start":1671395706715,"execution_millis":101,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Original Size (pruned model): 223.33 KB\nOriginal TFLite Size (pruned model): 44.02 KB\nZIP TFLite Size (pruned model): 20.64 KB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\n\noutput_dict = {\n    # 'timestamp': timestamp,\n    **PREPROCESSING_ARGS,\n    'filters' : FILTERS,\n    'final_sparsity' : final_sparsity,\n    'original_size' : original_model_size,\n    'tflite_size' : pruned_tflite_size,\n    'zip_size' : pruned_zip_size,\n    **TRAINING_ARGS,\n    'tflite_preprocessing_latency' : -1,\n    'tflite_model_latency' : -1,\n    'tflite_total_latency' : -1,\n    'original_test_accuracy': test_accuracy,\n    'tflite_test_accuracy' : -1\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./training_results.csv'\n# df.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","metadata":{"tags":[],"cell_id":"7ce9191d0b8b4d43984763499448ebee","source_hash":"73bd82d6","execution_start":1671395706722,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=919441e0-aa3f-4bac-a9dc-75daa4d8d934' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"45d3a544276547e1a7024955f0da1bdb","deepnote_execution_queue":[]}}